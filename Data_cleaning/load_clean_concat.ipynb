{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b747c15",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "689fda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from scipy import \n",
    "#from sklearn import\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import datetime\n",
    "import pywt\n",
    "# pd.set_option(\"display.max_rows\", 100)\n",
    "# from IPython.core.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1bfebb",
   "metadata": {},
   "source": [
    "### General variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e33594ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../Data/'\n",
    "print_text_result = False\n",
    "raw_data_names = ['pit1_data-2022', 'pit2_data-2022', 'pit3_data-2022', 'pit4_data-2022',\n",
    "                  'pit1_data-2023', 'pit2_data-2023', 'pit3_data-2023', 'pit4_data-2023']\n",
    "clean_data_names = ['VII_PIT1_2022', 'VII_PIT2_2022', 'VII_PIT3_2022', 'VII_PIT4_2022']\n",
    "df_pairs = [('pit1_data-2022', 'VII_PIT1_2022'), ('pit2_data-2022', 'VII_PIT2_2022'), ('pit3_data-2022', 'VII_PIT3_2022'), ('pit4_data-2022', 'VII_PIT4_2022'),\n",
    "            ('pit1_data-2023', None), ('pit2_data-2023', None), ('pit3_data-2023', None), ('pit4_data-2023', None)]\n",
    "col_types = ['int64', 'int64', 'int64', 'int64', 'int64', 'float64', 'int64', 'float64', 'int64', 'float64',\n",
    "             'int64', 'float64', 'int64', 'float64', 'int64', 'float64', 'float64', 'int64', 'float64', 'float64',\n",
    "             'float64', 'float64', 'int64', 'float64', 'float64', 'float64', 'float64', 'float64', 'bool']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62b14e",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a471267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_result(header: str, init_count: int, new_count: int):\n",
    "    print(header)\n",
    "    print(f'\\tinit row cnt: {init_count}')\n",
    "    print(f'\\t# of rows deleted: {init_count - new_count}')\n",
    "    print(f'\\tresult row count : {new_count}')\n",
    "\n",
    "def remove_pit_suffix(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove suffix '_pit<number>' from header\n",
    "    \"\"\"\n",
    "    re_match = re.search(r'_pit\\d+$', name)\n",
    "    if re_match:\n",
    "        name = name[:re_match.start()]\n",
    "    return name\n",
    "\n",
    "def prune_unnecessary_features(df: pd.DataFrame):\n",
    "    unnecessary_features = ['Temp_T21_Avg(1)','Temp_T21_Avg(2)', 'Temp_T21_Avg(3)', 'Temp_T21_Avg(4)', 'Temp_T21_Avg(5)',\n",
    "                            'CCVWC_Avg(1)', 'CCVWC_Avg(2)', 'CCVWC_Avg(3)', 'CCVWC_Avg(4)', 'CCVWC_Avg(5)', 'shf_plate_Avg',\n",
    "                            'shf_multiplier', 'shf_htr_resstnc', 'shfp_wrnng_flg', 'btt_wrnng_flg', 'PTemp_C_Avg', 'RECORD', 'BattV_Min']\n",
    "    df.drop(unnecessary_features, axis=1, inplace=True)\n",
    "\n",
    "def filter_dates(df: pd.DataFrame, file_name: str) -> int:\n",
    "    init_row_cnt = len(df)\n",
    "    start_date_2023 = datetime.datetime(2023, 1,1, 0, 0, 0)\n",
    "\n",
    "    if '2022' in file_name:\n",
    "        df.drop(df[df['TIMESTAMP'] >= start_date_2023].index, inplace=True)\n",
    "    else:\n",
    "        df.drop(df[df['TIMESTAMP'] < start_date_2023].index, inplace=True)\n",
    "\n",
    "    if print_text_result: print_result('FILTERING DATES BY YEAR:', init_row_cnt, len(df))\n",
    "\n",
    "    return (init_row_cnt - len(df))\n",
    "\n",
    "def filter_duplicate_date(df: pd.DataFrame) -> int:\n",
    "    init_row_cnt = len(df)\n",
    "    \n",
    "    df.drop_duplicates(subset='TIMESTAMP', inplace=True)\n",
    "\n",
    "    if print_text_result: print_result('FILTERING DUPLICATE DATES:', init_row_cnt, len(df))\n",
    "\n",
    "    return (init_row_cnt - len(df))\n",
    "\n",
    "def calculate_wavelet_coefficients(df: pd.DataFrame, period_lower_bound: float, period_upper_bound: float, file_name: str) -> None:\n",
    "    \n",
    "    # period of the wave T is calculated in days, frequency = 1/T \n",
    "    dt = (datetime.datetime(2023,1,1,0,5,0) - datetime.datetime(2023,1,1,0,0,0)).seconds / (60 * 60 * 24)\n",
    "\n",
    "    if 'VII' in file_name:\n",
    "        return df\n",
    "\n",
    "    for sensor in np.arange(5):\n",
    "        redox_series = \"Redox_Avg(\" + str(sensor + 1) + \")\"     \n",
    "        scales = np.geomspace(1, 2400, 30)\n",
    "        signal = df[redox_series]\n",
    "        [coefficients, frequencies] = pywt.cwt(signal, scales, \"cmor1.5-0.5\", dt)\n",
    "        power = abs(coefficients)\n",
    "        periods = 1 / frequencies\n",
    "        coef_idx = np.where((periods >= period_lower_bound) & (periods <= period_upper_bound), True, False)\n",
    "        power = power[np.arange(len(frequencies))[coef_idx], :].T\n",
    "        wavelet_cols = [\"Wave_period_\" + str(round(period,1)) + \"(\" + str(sensor + 1) + \")\" for period in periods[np.arange(len(frequencies))[coef_idx]]]\n",
    "        wavelet_df = pd.DataFrame(power, columns = wavelet_cols, index = df.index)\n",
    "        df = pd.concat([df, wavelet_df], axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fill_na_redox_values(df: pd.DataFrame, file_name: str) -> None:\n",
    "    \n",
    "    if 'VII' in file_name:\n",
    "        return df\n",
    "\n",
    "    for sensor in np.arange(5):\n",
    "        redox_series = \"Redox_Avg(\" + str(sensor + 1) + \")\"\n",
    "        df[redox_series] = df[redox_series].ffill()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def filter_missing_values(df: pd.DataFrame, file_name: str, remove_empy=True) -> int:\n",
    "    init_row_cnt = df.shape[0]\n",
    "    if remove_empy and 'VII' not in file_name:\n",
    "        df.dropna(inplace=True)\n",
    "    # TODO else:\n",
    "        # filter cleaned data\n",
    "        # fill missing data\n",
    "\n",
    "    if print_text_result: print_result('FILTERING MISSING VALUES:', init_row_cnt, len(df))\n",
    "\n",
    "    return (init_row_cnt - len(df))\n",
    "\n",
    "def filter_df(df: pd.DataFrame, file_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Filters the given DataFrame by removing specific rows\n",
    "    \"\"\"\n",
    "    diff_dict = dict()\n",
    "    prune_unnecessary_features(df)\n",
    "    diff_dict['dates_removed'] = filter_dates(df, file_name)\n",
    "    diff_dict['duplicates_removed'] = filter_duplicate_date(df)\n",
    "    df = calculate_wavelet_coefficients(df, 1/2, 5, file_name)\n",
    "    diff_dict['missing_data_removed'] = filter_missing_values(df, file_name)\n",
    "    return diff_dict\n",
    "\n",
    "def fix_types(df: pd.DataFrame, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fix data types from given DataFrame\n",
    "    \"\"\"\n",
    "    if 'VII' in file_name:\n",
    "        return df\n",
    "    for col_type, col_name in zip(col_types, list(df.columns.array)[1:]):\n",
    "        if col_type == 'int64':\n",
    "            df[col_name] = df[col_name].astype('float64').astype('int64')\n",
    "        elif col_type == 'float64':\n",
    "            df[col_name] = df[col_name].astype('float64')\n",
    "        else:\n",
    "            df[col_name] = df[col_name].astype('bool')\n",
    "    return df\n",
    "\n",
    "def add_pit_column(df: pd.DataFrame, file_name: str) -> pd.DataFrame:\n",
    "    if 'VII' in file_name:\n",
    "        return df\n",
    "    pit_n = file_name[3]\n",
    "    df['pit_number'] = int(pit_n)\n",
    "    return df\n",
    "\n",
    "def get_time_windows(hour_prediods: list[int]):\n",
    "    return [int((period_h*60)/5) for period_h in hour_prediods]\n",
    "\n",
    "def timestamp_gap(df: pd.DataFrame, td: timedelta, print_stats_graphs: bool = True) -> tuple: \n",
    "    df[\"TIMESTAMP_DIFF\"] = df.loc[:, \"TIMESTAMP\"].diff()\n",
    "    \n",
    "    breaks = list(df.index[(df[\"TIMESTAMP_DIFF\"] != td) & (np.isnan(df[\"TIMESTAMP_DIFF\"]) == False)])\n",
    "    gaps = [int(df.loc[index, \"TIMESTAMP_DIFF\"].total_seconds()/60) for index in breaks]\n",
    "    \n",
    "    gaps_observations = pd.DataFrame([[index, gap] for index, gap in zip(breaks, gaps)])\n",
    "    if gaps_observations.empty == False:\n",
    "        gaps_observations.columns = [\"Observation index\", \"Time delta (min)\"]\n",
    "        \n",
    "        if print_stats_graphs == True:\n",
    "            print(f\"the observations having gaps\")\n",
    "            print(gaps_observations)\n",
    "    else: \n",
    "        if print_stats_graphs == True:\n",
    "            print(f\"there are no gaps\")\n",
    "    return (breaks, gaps)\n",
    "\n",
    "def calculate_backward_sigma(rolling_window: pd.DataFrame, breaks: list, window_size: int, starting_index: int):\n",
    "    if rolling_window.index[-1] < starting_index + window_size-1:\n",
    "        return([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "    for j in breaks:\n",
    "        break_zero_index = rolling_window.index[-1] - j\n",
    "        if (break_zero_index >=0) and (break_zero_index < window_size-1):\n",
    "            return([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "    return rolling_window[[\"Redox_Avg(1)\", \"Redox_Avg(2)\", \"Redox_Avg(3)\", \"Redox_Avg(4)\", \"Redox_Avg(5)\"]].apply(func=np.std, axis=0)\n",
    "\n",
    "def sigma_feature_engineering(df: pd.DataFrame, window_sizes, td, print_sigma_statistics = False):\n",
    "    breaks = timestamp_gap(df, td)[0]\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        widow_h = int((window_size*5)/60)\n",
    "        for sensor in range(1,6):\n",
    "            df[f'Redox_Avg({sensor})_sigma_b_{widow_h}'] = np.nan\n",
    "            df[f'Redox_Avg({sensor})_sigma_f_{widow_h}'] = np.nan\n",
    "        \n",
    "        backward_sigma = np.array([calculate_backward_sigma(rolling_window, breaks = breaks, window_size = window_size, starting_index = df.index[0]) for rolling_window in df.rolling(window_size)])\n",
    "\n",
    "        df[[f'Redox_Avg(1)_sigma_b_{widow_h}', f'Redox_Avg(2)_sigma_b_{widow_h}', f'Redox_Avg(3)_sigma_b_{widow_h}', f'Redox_Avg(4)_sigma_b_{widow_h}', f'Redox_Avg(5)_sigma_b_{widow_h}']] = backward_sigma\n",
    "        \n",
    "        forward_sigma = df.loc[:, [f'Redox_Avg(1)_sigma_b_{widow_h}', f'Redox_Avg(2)_sigma_b_{widow_h}', f'Redox_Avg(3)_sigma_b_{widow_h}', f'Redox_Avg(4)_sigma_b_{widow_h}', f'Redox_Avg(5)_sigma_b_{widow_h}']].shift(-(window_size-1))\n",
    "        df[[f'Redox_Avg(1)_sigma_f_{widow_h}', f'Redox_Avg(2)_sigma_f_{widow_h}', f'Redox_Avg(3)_sigma_f_{widow_h}', f'Redox_Avg(4)_sigma_f_{widow_h}', f'Redox_Avg(5)_sigma_f_{widow_h}']] = forward_sigma.to_numpy()\n",
    "\n",
    "        if print_sigma_statistics == True:\n",
    "            print(f\"Standard deviation statistics are calculated for the window size {window_size}\")\n",
    "            print(df[f'Redox_Avg(1)_sigma_b_{widow_h}', f'Redox_Avg(2)_sigma_b_{widow_h}', f'Redox_Avg(3)_sigma_b_{widow_h}', f'Redox_Avg(4)_sigma_b_{widow_h}', f'Redox_Avg(5)_sigma_b_{widow_h}'].apply([min, max, np.mean, np.median], axis = 0).to_string())\n",
    "\n",
    "    sigma_col_names_list = [col_name for col_name in df.columns.array if 'sigma' in col_name]\n",
    "    removed_rows = df.index[np.any(df.loc[:, sigma_col_names_list].isna(), axis=1)]\n",
    "\n",
    "    df.drop(removed_rows, axis=0, inplace=True)\n",
    "\n",
    "    print(f\"{len(removed_rows)} rows had to be removed due to \\\" hitting \\\" time gaps or margin areas when calculating standard deviation\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_raw_cleaned(raw_dfs: dict[str, pd.DataFrame], clean_dfs: dict[str, pd.DataFrame], df_pairs: list[(str, str)] = df_pairs):\n",
    "    merged_dfs = []\n",
    "    for pair in df_pairs:\n",
    "        raw = raw_dfs[pair[0]]\n",
    "        if pair[1] is None:\n",
    "            for x in range(1,6):\n",
    "                raw[f'Redox_error_flag({x})'] = False\n",
    "            raw['Redox_error_flag_available'] = False\n",
    "            merged_dfs.append(raw)\n",
    "        else:\n",
    "            cleaned = clean_dfs[pair[1]]\n",
    "            for x in range(1,6):\n",
    "                cleaned[f'Redox_error_flag({x})'] = ((cleaned['Redox_error_flag'] == True) & (cleaned[f'Redox_Avg({x})'].isna() == False))\n",
    "            # cleaned = cleaned.drop(['Redox_error_flag'], axis=1)\n",
    "            merged = raw.merge(\n",
    "                cleaned[['TIMESTAMP', 'Redox_error_flag(1)', 'Redox_error_flag(2)', 'Redox_error_flag(3)', 'Redox_error_flag(4)', 'Redox_error_flag(5)', 'Redox_error_flag']],\n",
    "                how='left',\n",
    "                left_on='TIMESTAMP',\n",
    "                right_on='TIMESTAMP'\n",
    "            )\n",
    "            merged['Redox_error_flag_available'] = True\n",
    "            merged_dfs.append(merged)\n",
    "    return merged_dfs\n",
    "\n",
    "def add_redox_log_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for x in range(1,6):\n",
    "        df[f'log_redox({x})'] = np.log(df[f'Redox_Avg({x})'])\n",
    "    return df\n",
    "\n",
    "def load_data(file_names: list[str], data_folder: str) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load data from given file names\n",
    "    :param file_names: list of file names\n",
    "    :param data_folder: folder name where data is located\n",
    "    :return: dictionary of DataFrames\n",
    "    \"\"\"\n",
    "    dfs = dict()\n",
    "    report_df = pd.DataFrame()\n",
    "\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n",
    "        df.rename(mapper=remove_pit_suffix, axis='columns', inplace=True)\n",
    "        \n",
    "        if print_text_result: print(f'===== {file_name} =====')\n",
    "        \n",
    "        diff_dict = dict()\n",
    "        prune_unnecessary_features(df)\n",
    "        \n",
    "        diff_dict['dates_removed'] = filter_dates(df, file_name)\n",
    "        diff_dict['duplicates_removed'] = filter_duplicate_date(df)\n",
    "        \n",
    "        # here is where the wavelet coefficients are calculated \n",
    "        df = fill_na_redox_values(df, file_name)\n",
    "        df = calculate_wavelet_coefficients(df, 1/2, 5, file_name)\n",
    "        \n",
    "        window_sizes = get_time_windows([12, 24])\n",
    "        \n",
    "        td = timedelta(days = 0, hours = 0, minutes = 5, seconds = 0, milliseconds= 0, weeks = 0)\n",
    "        \n",
    "        if 'VII' not in file_name:\n",
    "            df = sigma_feature_engineering(df, window_sizes=window_sizes, td=td)\n",
    "        \n",
    "        diff_dict['missing_data_removed'] = filter_missing_values(df, file_name)\n",
    "\n",
    "        # instead of bundling the filtering functions they are applied one by one as the wavelet calculations wedged in \n",
    "        # stats = filter_df(df, file_name)\n",
    "\n",
    "        # TODO add log scale to redox and error flag for each redox depth\n",
    "        df = fix_types(df, file_name)\n",
    "        df = add_pit_column(df, file_name)\n",
    "        # df = add_redox_log_cols(df)\n",
    "        report_df = pd.concat([report_df, pd.DataFrame(diff_dict, index=[file_name])])\n",
    "        dfs[file_name] = df\n",
    "\n",
    "        if print_text_result: print('\\n')\n",
    "\n",
    "    report_df = report_df.assign(Total = lambda x: (x.sum(axis=1)))\n",
    "    print(report_df)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215119e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "425335a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (34,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the observations having gaps\n",
      "   Observation index  Time delta (min)\n",
      "0              47113              1445\n",
      "1148 rows had to be removed due to \" hitting \" time gaps or margin areas when calculating standard deviation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are no gaps\n",
      "574 rows had to be removed due to \" hitting \" time gaps or margin areas when calculating standard deviation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are no gaps\n",
      "574 rows had to be removed due to \" hitting \" time gaps or margin areas when calculating standard deviation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the observations having gaps\n",
      "   Observation index  Time delta (min)\n",
      "0               2016              2045\n",
      "1148 rows had to be removed due to \" hitting \" time gaps or margin areas when calculating standard deviation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (34,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the observations having gaps\n",
      "   Observation index  Time delta (min)\n",
      "0              23270              1010\n",
      "1              25322              1520\n",
      "1722 rows had to be removed due to \" hitting \" time gaps or margin areas when calculating standard deviation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (7,8,9,34,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the observations having gaps\n",
      "   Observation index  Time delta (min)\n",
      "0              54325              1395\n",
      "1             108172                10\n",
      "2             115697                20\n",
      "2296 rows had to be removed due to \" hitting \" time gaps or margin areas when calculating standard deviation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the observations having gaps\n",
      "   Observation index  Time delta (min)\n",
      "0              87552                10\n",
      "1148 rows had to be removed due to \" hitting \" time gaps or margin areas when calculating standard deviation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (19,20,21,30,31,34,37,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the observations having gaps\n",
      "   Observation index  Time delta (min)\n",
      "0              39102                10\n",
      "1              69832                60\n",
      "1722 rows had to be removed due to \" hitting \" time gaps or margin areas when calculating standard deviation\n",
      "                dates_removed  duplicates_removed  missing_data_removed  Total\n",
      "pit1_data-2022              1                  12                     0     13\n",
      "pit2_data-2022              1                  24                  4057   4082\n",
      "pit3_data-2022              1                  12                     0     13\n",
      "pit4_data-2022              1                  12                     0     13\n",
      "pit1_data-2023            287                  15                     0    302\n",
      "pit2_data-2023           3248               25552                  6868  35668\n",
      "pit3_data-2023            287                  88                     0    375\n",
      "pit4_data-2023            287                 196                     4    487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (37,38,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n",
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n",
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (37,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               dates_removed  duplicates_removed  missing_data_removed  Total\n",
      "VII_PIT1_2022              1                 192                     0    193\n",
      "VII_PIT2_2022              1                  24                     0     25\n",
      "VII_PIT3_2022              1                  12                     0     13\n",
      "VII_PIT4_2022              1                  12                     0     13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/nb86kq1j7zx8sqjthk_hy6vc0000gn/T/ipykernel_76761/1859660079.py:223: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_folder+file_name+'.csv', parse_dates=['TIMESTAMP'])\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data(raw_data_names, data_folder)\n",
    "clean_data = load_data(clean_data_names, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279fe4a",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4598ba95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>Redox_Avg(1)</th>\n",
       "      <th>Redox_Avg(2)</th>\n",
       "      <th>Redox_Avg(3)</th>\n",
       "      <th>Redox_Avg(4)</th>\n",
       "      <th>Redox_Avg(5)</th>\n",
       "      <th>Temp_T12_Avg(1)</th>\n",
       "      <th>EC_Avg(1)</th>\n",
       "      <th>Temp_T12_Avg(2)</th>\n",
       "      <th>EC_Avg(2)</th>\n",
       "      <th>...</th>\n",
       "      <th>Redox_Avg(1)_sigma_f_24</th>\n",
       "      <th>Redox_Avg(2)_sigma_b_24</th>\n",
       "      <th>Redox_Avg(2)_sigma_f_24</th>\n",
       "      <th>Redox_Avg(3)_sigma_b_24</th>\n",
       "      <th>Redox_Avg(3)_sigma_f_24</th>\n",
       "      <th>Redox_Avg(4)_sigma_b_24</th>\n",
       "      <th>Redox_Avg(4)_sigma_f_24</th>\n",
       "      <th>Redox_Avg(5)_sigma_b_24</th>\n",
       "      <th>Redox_Avg(5)_sigma_f_24</th>\n",
       "      <th>pit_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>2022-04-13 08:55:00</td>\n",
       "      <td>134</td>\n",
       "      <td>298</td>\n",
       "      <td>160</td>\n",
       "      <td>77</td>\n",
       "      <td>56</td>\n",
       "      <td>0.2</td>\n",
       "      <td>84</td>\n",
       "      <td>0.3</td>\n",
       "      <td>392</td>\n",
       "      <td>...</td>\n",
       "      <td>3.165684</td>\n",
       "      <td>1.099647</td>\n",
       "      <td>1.796422</td>\n",
       "      <td>4.505482</td>\n",
       "      <td>5.023514</td>\n",
       "      <td>2.651380</td>\n",
       "      <td>3.425405</td>\n",
       "      <td>1.620081</td>\n",
       "      <td>2.512996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2022-04-13 09:00:00</td>\n",
       "      <td>134</td>\n",
       "      <td>298</td>\n",
       "      <td>160</td>\n",
       "      <td>77</td>\n",
       "      <td>57</td>\n",
       "      <td>0.2</td>\n",
       "      <td>84</td>\n",
       "      <td>0.3</td>\n",
       "      <td>392</td>\n",
       "      <td>...</td>\n",
       "      <td>3.171434</td>\n",
       "      <td>1.098864</td>\n",
       "      <td>1.795783</td>\n",
       "      <td>4.504725</td>\n",
       "      <td>5.022991</td>\n",
       "      <td>2.653856</td>\n",
       "      <td>3.420524</td>\n",
       "      <td>1.620306</td>\n",
       "      <td>2.514988</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2022-04-13 09:05:00</td>\n",
       "      <td>133</td>\n",
       "      <td>298</td>\n",
       "      <td>160</td>\n",
       "      <td>76</td>\n",
       "      <td>57</td>\n",
       "      <td>0.2</td>\n",
       "      <td>84</td>\n",
       "      <td>0.3</td>\n",
       "      <td>392</td>\n",
       "      <td>...</td>\n",
       "      <td>3.177007</td>\n",
       "      <td>1.098478</td>\n",
       "      <td>1.794928</td>\n",
       "      <td>4.504939</td>\n",
       "      <td>5.020418</td>\n",
       "      <td>2.656595</td>\n",
       "      <td>3.415380</td>\n",
       "      <td>1.620866</td>\n",
       "      <td>2.516280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2022-04-13 09:10:00</td>\n",
       "      <td>133</td>\n",
       "      <td>298</td>\n",
       "      <td>160</td>\n",
       "      <td>77</td>\n",
       "      <td>56</td>\n",
       "      <td>0.2</td>\n",
       "      <td>84</td>\n",
       "      <td>0.3</td>\n",
       "      <td>392</td>\n",
       "      <td>...</td>\n",
       "      <td>3.184965</td>\n",
       "      <td>1.097437</td>\n",
       "      <td>1.794410</td>\n",
       "      <td>4.502925</td>\n",
       "      <td>5.019582</td>\n",
       "      <td>2.657446</td>\n",
       "      <td>3.411222</td>\n",
       "      <td>1.622338</td>\n",
       "      <td>2.516780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>2022-04-13 09:15:00</td>\n",
       "      <td>133</td>\n",
       "      <td>298</td>\n",
       "      <td>160</td>\n",
       "      <td>77</td>\n",
       "      <td>56</td>\n",
       "      <td>0.2</td>\n",
       "      <td>84</td>\n",
       "      <td>0.3</td>\n",
       "      <td>392</td>\n",
       "      <td>...</td>\n",
       "      <td>3.191444</td>\n",
       "      <td>1.096121</td>\n",
       "      <td>1.793165</td>\n",
       "      <td>4.503148</td>\n",
       "      <td>5.016432</td>\n",
       "      <td>2.659561</td>\n",
       "      <td>3.405667</td>\n",
       "      <td>1.623102</td>\n",
       "      <td>2.518526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              TIMESTAMP  Redox_Avg(1)  Redox_Avg(2)  Redox_Avg(3)  \\\n",
       "287 2022-04-13 08:55:00           134           298           160   \n",
       "288 2022-04-13 09:00:00           134           298           160   \n",
       "289 2022-04-13 09:05:00           133           298           160   \n",
       "290 2022-04-13 09:10:00           133           298           160   \n",
       "291 2022-04-13 09:15:00           133           298           160   \n",
       "\n",
       "     Redox_Avg(4)  Redox_Avg(5)  Temp_T12_Avg(1)  EC_Avg(1)  Temp_T12_Avg(2)  \\\n",
       "287            77            56              0.2         84              0.3   \n",
       "288            77            57              0.2         84              0.3   \n",
       "289            76            57              0.2         84              0.3   \n",
       "290            77            56              0.2         84              0.3   \n",
       "291            77            56              0.2         84              0.3   \n",
       "\n",
       "     EC_Avg(2)  ...  Redox_Avg(1)_sigma_f_24  Redox_Avg(2)_sigma_b_24  \\\n",
       "287        392  ...                 3.165684                 1.099647   \n",
       "288        392  ...                 3.171434                 1.098864   \n",
       "289        392  ...                 3.177007                 1.098478   \n",
       "290        392  ...                 3.184965                 1.097437   \n",
       "291        392  ...                 3.191444                 1.096121   \n",
       "\n",
       "     Redox_Avg(2)_sigma_f_24  Redox_Avg(3)_sigma_b_24  \\\n",
       "287                 1.796422                 4.505482   \n",
       "288                 1.795783                 4.504725   \n",
       "289                 1.794928                 4.504939   \n",
       "290                 1.794410                 4.502925   \n",
       "291                 1.793165                 4.503148   \n",
       "\n",
       "     Redox_Avg(3)_sigma_f_24  Redox_Avg(4)_sigma_b_24  \\\n",
       "287                 5.023514                 2.651380   \n",
       "288                 5.022991                 2.653856   \n",
       "289                 5.020418                 2.656595   \n",
       "290                 5.019582                 2.657446   \n",
       "291                 5.016432                 2.659561   \n",
       "\n",
       "     Redox_Avg(4)_sigma_f_24  Redox_Avg(5)_sigma_b_24  \\\n",
       "287                 3.425405                 1.620081   \n",
       "288                 3.420524                 1.620306   \n",
       "289                 3.415380                 1.620866   \n",
       "290                 3.411222                 1.622338   \n",
       "291                 3.405667                 1.623102   \n",
       "\n",
       "     Redox_Avg(5)_sigma_f_24  pit_number  \n",
       "287                 2.512996           1  \n",
       "288                 2.514988           1  \n",
       "289                 2.516280           1  \n",
       "290                 2.516780           1  \n",
       "291                 2.518526           1  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['pit1_data-2022'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262bc7dd",
   "metadata": {},
   "source": [
    "### Dtypes in each dataframe\n",
    "\n",
    "    NOTE: Cleaned data dtypes not changed yet. Need to think how to deal with missing values for column which should be converted to int64 from original float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72a7a6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t1\t\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\t24\t25\t26\t27\t28\t29\t30\t31\t32\t33\t34\t35\t36\t37\t38\t39\t40\t41\t42\t43\t44\t45\t46\t47\t48\n",
      "pit1_data-2022\t1\tdatetime64[ns]\tint64\tint64\tint64\tint64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tbool\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\ttimedelta64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\n",
      "pit2_data-2022\t2\tdatetime64[ns]\tint64\tint64\tint64\tint64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tbool\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\ttimedelta64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\n",
      "pit3_data-2022\t3\tdatetime64[ns]\tint64\tint64\tint64\tint64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tbool\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\ttimedelta64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\n",
      "pit4_data-2022\t4\tdatetime64[ns]\tint64\tint64\tint64\tint64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tbool\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\ttimedelta64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\n",
      "pit1_data-2023\t5\tdatetime64[ns]\tint64\tint64\tint64\tint64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tbool\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\ttimedelta64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\n",
      "pit2_data-2023\t6\tdatetime64[ns]\tint64\tint64\tint64\tint64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tbool\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\ttimedelta64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\n",
      "pit3_data-2023\t7\tdatetime64[ns]\tint64\tint64\tint64\tint64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tbool\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\ttimedelta64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\n",
      "pit4_data-2023\t8\tdatetime64[ns]\tint64\tint64\tint64\tint64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tint64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tbool\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\ttimedelta64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\n",
      "VII_PIT1_2022\t9\tdatetime64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tobject\n",
      "VII_PIT2_2022\t10\tdatetime64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tobject\n",
      "VII_PIT3_2022\t11\tdatetime64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tobject\n",
      "VII_PIT4_2022\t12\tdatetime64[ns]\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tobject\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print('\\t\\t\\t'+'1\\t\\t'+'\\t'.join(str(a) for a in [*range(2,49)]))\n",
    "for t1 in zip([*raw_data.items(), *clean_data.items()]):\n",
    "    print(f'{t1[0][0]}\\t{i}\\t'+'\\t'.join(str(x) for x in t1[0][1].dtypes.array))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b90782",
   "metadata": {},
   "source": [
    "### Remove timestamps from raw data that do not match in cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b52341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking pit1_data-2022 vs VII_PIT1_2022\n",
      "\t Rows removed 0\n",
      "Checking pit2_data-2022 vs VII_PIT2_2022\n",
      "\t Rows removed 0\n",
      "Checking pit3_data-2022 vs VII_PIT3_2022\n",
      "\t Rows removed 0\n",
      "Checking pit4_data-2022 vs VII_PIT4_2022\n",
      "\t Rows removed 1442\n"
     ]
    }
   ],
   "source": [
    "for pair in df_pairs:\n",
    "    raw_data_name, clean_data_name = pair[0], pair[1]\n",
    "    if clean_data_name:\n",
    "        print(f'Checking {raw_data_name} vs {clean_data_name}')\n",
    "        clean_timestamps = clean_data[clean_data_name]['TIMESTAMP'].to_numpy()\n",
    "        prev_row_count = len(raw_data[raw_data_name])\n",
    "        raw_data[raw_data_name] = raw_data[raw_data_name].loc[raw_data[raw_data_name]['TIMESTAMP'].isin(clean_timestamps) == True]\n",
    "        print(f'\\t Rows removed {prev_row_count-len(raw_data[raw_data_name])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30da575",
   "metadata": {},
   "source": [
    "### Add redox error flag columns and merge them with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83d5df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfs = merge_raw_cleaned(raw_data, clean_data, df_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89496cb7",
   "metadata": {},
   "source": [
    "### Combine all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41b712dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_raw_data_df = pd.DataFrame()\n",
    "for df in merged_dfs:\n",
    "    all_raw_data_df = pd.concat([all_raw_data_df, df], ignore_index=True)\n",
    "all_raw_data_df['Redox_error_flag'] = all_raw_data_df['Redox_error_flag'].fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d7c17",
   "metadata": {},
   "source": [
    "### Save data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3557eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder_path = f'{data_folder}/Training/'\n",
    "all_raw_data_df.to_csv(f'{training_folder_path}Raw_training_data_full.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
